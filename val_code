import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count
import logging



def is_valid_email(email):
    pattern = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'
    return bool(re.match(pattern, email))

print(is_valid_email("john.doe@gmail.com"))


logging.basicConfig(level=logging.INFO)

spark = SparkSession.builder.appName("DuplicateValidation").getOrCreate()

def validate_duplicates_spark(file_path, key_cols):
    try:
        # Step 1: Read file
        df = spark.read.option("header", True).csv(file_path)
        logging.info("File read successfully")

        # Step 2: Validate schema
        for col_name in key_cols:
            if col_name not in df.columns:
                raise ValueError(f"Missing column: {col_name}")

        # Step 3: Find duplicate keys
        dup_keys = df.groupBy(key_cols) \
                     .agg(count("*").alias("cnt")) \
                     .filter(col("cnt") > 1)

        dup_count = dup_keys.count()

        if dup_count > 0:
            logging.warning(f"Duplicate keys found: {dup_count}")

            # Step 4: Extract duplicate records
            df_duplicates = df.join(dup_keys, key_cols, "inner")

            # Step 5: Save to audit path
            df_duplicates.write.mode("overwrite").parquet("output/duplicates/")
            logging.info("Duplicates written to output/duplicates/")

            raise Exception(f"Duplicate data found: {dup_count} keys")

        logging.info("No duplicates found")

        return df

    except ValueError as ve:
        logging.error(f"Schema validation error: {ve}")
        print(ve)

    except Exception as e:
        logging.error(f"Processing error: {e}")
        print(e)

    finally:
        logging.info("Spark validation job completed")

# Usage
#validate_duplicates_spark("input.csv", ["customer_id"])